#!/bin/bash

# Check for required arguments
if [ $# -ne 3 ]; then
    echo "Usage: $0 <teacher_model_id> <student_model_id> <benchmark_json>"
    exit 1
fi

# Assign command-line arguments
teacher_model_id=$1
student_model_id=$2
data_path=$3

# Creating dir with teacher, student model name and dataset path. All files saved in this dir.
sanitize_benchmark_name=$(basename "$data_path" .json)
sanitize_teacher_model_id=${teacher_model_id//\//-}
sanitize_student_model_id=${student_model_id//\//-}

pipeline_run_dir="pipeline_run_${sanitize_teacher_model_id}_${sanitize_student_model_id}_${sanitize_benchmark_name}"

mkdir -p "$pipeline_run_dir"
echo "Created pipeline run directory: $pipeline_run_dir"

cot_data_path="${pipeline_run_dir}/cot_pipeline_data_dir/cot_finetune_split.json"
cot_train_data_path="${pipeline_run_dir}/cot_pipeline_data_dir/cot_finetune_train_split.json"
cot_val_data_path="${pipeline_run_dir}/cot_pipeline_data_dir/cot_finetune_val_split.json"
cot_test_data_path="${pipeline_run_dir}/cot_pipeline_data_dir/cot_finetune_test_split.json"

teacher_lora_train_data_path="${pipeline_run_dir}/cot_pipeline_data_dir/teacher_lora_finetune_split.json"

images_dir="/gpfs/home/ym621/gavin/Hypo3D/hypo_dataset/dataset/top_view_with_label_rotated"

# Path to cot_gen file that has CoT generated for training dataset
cot_gen_data_path="${pipeline_run_dir}/${teacher_model_id//\//_}_cot_gen_train_data.json"

# Path to teacher model generated logits
teacher_cached_logits_dir="${pipeline_run_dir}/teacher_cache/"

# Standard LoRA adapter attached to teacher model
teacher_standard_lora_adapter="${pipeline_run_dir}/lora_finetuned_${teacher_model_id//\//_}"
student_standard_lora_adapter="${pipeline_run_dir}/lora_finetuned_${student_model_id//\//_}"
student_cot_lora_adapter="${pipeline_run_dir}/lora_distilled_${student_model_id//\//_}"

# Parse model name for model ID to determine which model specific evaluate and fine-tune scripts to use
model_id=$(echo "$teacher_model_id" | tr '[:upper:]' '[:lower:]')

if [[ "$model_id" == *"llava-onevision"* ]]; then
    model_dir_path="llava-ov"
elif [[ "$model_id" == *"qwen2-vl"* ]]; then
    model_dir_path="Qwen2-VL"
else 
    echo "Unrecognized model type"
    exit 1
fi

# Check if the file exists before running the script
if [[ -f "$data_path" ]]; then
    echo "Processing file: $data_path"

    ##############################
    # does not work, try 7b as teacher and 2b as student

    echo "----PIPELINE START----"

    echo "----PRE-PROCESSING STAGE:--------"

    echo "Splitting benchmark dataset into 0.8/0.2 splits for cot_pipeline and teacher LoRA fine-tuning, respectively"
    python hypo_dataset/dataset_split.py 0.8 0.0 0.2 $data_path 1 $pipeline_run_dir    

    echo "Splitting 0.8 cot_pipeline_data into train, validation and test splits, using ratio: 0.7, 0.1, 0.2"
    python hypo_dataset/dataset_split.py 0.7 0.02 0.28 $cot_data_path 2 $pipeline_run_dir

    echo "Teacher model: Zero shot configuration performance on test set"
    python 2D-VLM/$model_dir_path/evaluate.py \
        -f "$cot_test_data_path" \
        -m "$teacher_model_id" \
        -p "$pipeline_run_dir" \

    echo "Teacher model: Adapter tuning (generate LoRA adapter) with cross entropy loss function. Trained on 0.2 split that is different from cot train_split used in rest of pipeline."
    python 2D-VLM/$model_dir_path/normal_lora_finetuning.py \
        -f "$teacher_lora_train_data_path" \
        -m "$teacher_model_id" \
        -i "$images_dir" \
        -e "$cot_val_data_path" \
        -p "$pipeline_run_dir" \

    echo "----------------------"

    echo "----COT-GEN STAGE:----"
    
    echo "Teacher model with standard LoRA adapter performing normal inference (no CoT generation)"
    python 2D-VLM/$model_dir_path/model_with_lora_evaluate.py \
        -f "$cot_train_data_path" \
        -m "$teacher_model_id" \
        -a "$teacher_standard_lora_adapter" \
        -p "$pipeline_run_dir"

    # Using student model's tokenizer to get vocab_size to match for the raw_logits generated.
    echo "Teacher model with normal LoRA adapter: Running inference on train dataset to generate CoT + raw logits. Generating CoT on unseen cot_train_data_split"
    python 2D-VLM/$model_dir_path/cot_gen.py \
        -f "$cot_train_data_path" \
        -m "$teacher_model_id" \
        -a "$teacher_standard_lora_adapter" \
        -s "$student_model_id" \
        -p "$pipeline_run_dir" \

    echo "----------------------"

    echo "----STUDENT-MODEL COT FINETUNING:----"

    # Support for Qwen2-VL CoT finetuning not finished yet
    if [[ "$model_dir_path" == "llava-ov" ]]; then
        echo "Student finetuning on CoT generated by teacher model"
        python 2D-VLM/llava-ov/cot_lora_finetuning.py \
            -f "$cot_gen_data_path" \
            -m "$student_model_id" \
            -i "$images_dir" \
            -e "$cot_val_data_path" \
            -c "$teacher_cached_logits_dir" \
            -p "$pipeline_run_dir" \
            -a "0.5"
    fi

    echo "----------------------"

    echo "----PERFORMANCE EVAL OF ZERO-SHOT, STANDARD LORA-FINETUNED, COT FINETUNED STUDENT MODEL----"

    echo "Running inference for zero-shot student model on test split to see zero-shot model performance"
    python 2D-VLM/$model_dir_path/bertscore_evaluate.py \
        -f "$cot_test_data_path" \
        -m "$student_model_id" \
        -p "$pipeline_run_dir" \

    echo "Student model finetuning standard LoRA to get adapter to determine standard LoRA student model finetuning performance"
    echo "Student model: Adapter tuning (generate LoRA adapter) with cross entropy loss function. Trained on 0.2 split that is different from cot train_split used in rest of pipeline."
    python 2D-VLM/$model_dir_path/normal_lora_finetuning.py \
        -f "$teacher_lora_train_data_path" \
        -m "$student_model_id" \
        -i "$images_dir" \
        -e "$cot_val_data_path" \
        -p "$pipeline_run_dir" \

    echo "Base/Zero-shot student model with standard finetuned LoRA Adapter performance evaluation on test split"
    python 2D-VLM/$model_dir_path/model_with_lora_evaluate.py \
        -f "$cot_test_data_path" \
        -m "$student_model_id" \
        -a "$student_cot_lora_adapter" \
        -p "$pipeline_run_dir" 

    # Support for Qwen2-VL CoT finetuning not finished yet
    if [[ "$model_dir_path" == "llava-ov" ]]; then 
        echo "Student model with LoRA adapter (finetuned on CoT + raw_logits from teacher model) performance evaluation"
        python 2D-VLM/llava-ov/model_with_lora_evaluate.py \
            -f "$cot_test_data_path" \
            -m "$student_model_id" \
            -a "$student_cot_lora_adapter" \
            -p "$pipeline_run_dir" 
    fi

    echo "----------------------"

    echo "----PIPELINE END----"

else
    echo "Error: File not found: $data_path"
fi
