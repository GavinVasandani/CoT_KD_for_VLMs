# LoRA finetuning using CoT and raw logits generated by teacher model

import os
import gc
import json
import torch
from PIL import Image
import argparse
import pandas as pd
import numpy as np
import transformers

from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback
from peft import LoraConfig, get_peft_model
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.nn import CrossEntropyLoss


# Function to extract all non-NaN values for each scene_id and return as a list
def extract_non_nan_values(df):
    extracted_values = {}
    for index, row in df.iterrows():
        scene_id = row['scene_id']
        if pd.notna(scene_id):
            scene_values = {}
            for direction in ['Front', 'Back', 'Left', 'Right']:
                value = row[direction]
                if pd.notna(value):
                    scene_values[direction] = value
            if scene_values:
                extracted_values[scene_id] = scene_values
    return extracted_values

# Function to pad or truncate incompatible tensor shape
def _pad_or_truncate(logits: torch.Tensor, length: int) -> torch.Tensor:
    seq_len, vocab_size = logits.size()
    if seq_len > length:
        return logits[:length]
    elif seq_len < length:
        pad_size = length - seq_len
        pad_tensor = logits.new_zeros((pad_size, vocab_size))
        return torch.cat([logits, pad_tensor], dim=0)
    else:
        return logits

# Define a simple Dataset to load training examples from your JSON file.
class Dataset(torch.utils.data.Dataset):
    def __init__(self, data_path, images_dir, processor, prompt_template, visual_obs_template, context_change_template, question_template, df, teacher_cache_dir, split, max_length=512):
        with open(data_path, "r") as f:
            raw_data = json.load(f)
        self.images_dir = images_dir
        self.processor = processor
        self.prompt_template = prompt_template
        self.visual_obs_template = visual_obs_template
        self.context_change_template = context_change_template
        self.question_template = question_template
        self.samples = []
        self.df = df
        self.max_length = max_length
        self.teacher_cache_dir = teacher_cache_dir
        self.split = split
        for scene_id, changes_list in raw_data.items():
            for change in changes_list:
                context_change = change["context_change"]
                for qa in change["questions_answers"]:
                    question = qa["question"]
                    answer = qa["answer"]
                    self.samples.append({
                        "scene_id": scene_id,
                        "context_change": context_change,
                        "question": question,
                        "answer": answer
                    })

        self.teacher_paths = [
            os.path.join(teacher_cache_dir, f"{i}.pt")
            for i in range(len(self.samples))
        ]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        scene_id = sample["scene_id"]
        image_path = os.path.join(self.images_dir, f"{scene_id}.png")
        # Open and convert image to RGB
        image = Image.open(image_path).convert("RGB")
        fixed_size = (384, 384)
        image = image.resize(fixed_size)

        scene_orientation = extract_non_nan_values(self.df[self.df['scene_id'] == scene_id])
        scene_orientation = " ".join(
            f"The {item} was located at the {direction.lower()} of the scene."
            for scene_id, directions in scene_orientation.items()
            for direction, item in directions.items()
        )

        change = sample["context_change"]
        question = sample["question"]

        # Visual observations prompt
        visual_prompt = self.visual_obs_template.format(
            orientation=scene_orientation
        )
        vis_msgs = [{"role":"user","content":[{"type":"image","image":image},{"type":"text","text":visual_prompt}]}]
        vis_input = self.processor.apply_chat_template(vis_msgs, add_generation_prompt=True)
        # Token ids of visual observations output
        vis_tokens = self.processor.tokenizer(vis_input, return_tensors="pt", padding=False, truncation=False)
        # Size of visual observations output (so number of tokens for vis obs output)
        L1 = vis_tokens.input_ids.size(1)

        # context change prompt
        change_prompt = self.context_change_template.format(model_vis_obs="<VISUAL_OBS>", change=change)
        chg_msgs = [{"role":"user","content":[{"type":"image","image":image},{"type":"text","text":change_prompt}]}]
        chg_input = self.processor.apply_chat_template(chg_msgs, add_generation_prompt=True)
        chg_tokens = self.processor.tokenizer(chg_input, return_tensors="pt", padding=False, truncation=False)
        L2 = chg_tokens.input_ids.size(1)
        
        # question prompt
        question_prompt = self.question_template.format(
            model_vis_obs="<VISUAL_OBS>", change=change,
            context_change_obs="<CHANGE_OBS>", question=question
        )
        ans_msgs = [{"role":"user","content":[{"type":"image","image":image},{"type":"text","text":question_prompt}]}]
        ans_input = self.processor.apply_chat_template(ans_msgs, add_generation_prompt=True)
        ans_tokens = self.processor.tokenizer(ans_input, return_tensors="pt", padding=False, truncation=False)
        L3 = ans_tokens.input_ids.size(1)

        # now tokenize full prompt + answer for labels and student inputs
        full_prompt = vis_input + chg_input + ans_input
        full_tokenized = self.processor.tokenizer(
            full_prompt, sample['answer'],
            return_tensors="pt", padding="max_length",
            truncation="only_second", max_length=self.max_length
        )
        for k,v in full_tokenized.items(): full_tokenized[k]=v.squeeze(0)
        prompt_len = self.processor.tokenizer(vis_input+chg_input+ans_input, return_tensors="pt")["input_ids"].size(1)
        labels = full_tokenized["input_ids"].clone()
        labels[:prompt_len] = -100
        full_tokenized["labels"] = labels

        if self.split == "train":
            q_num = idx+1
            base = os.path.join(self.teacher_cache_dir, str(q_num))
            # Load visual_observations field raw_logits generated by teacher
            vis = torch.load(f"{base}/visual_observations.pt")['logits']
            # Load context_change field raw_logits generated by teacher
            chg = torch.load(f"{base}/context_change.pt")['logits'] 
            # Load answer field raw_logits generated by teacher
            ans = torch.load(f"{base}/answer.pt")['logits']

            # pad/truncate teacher output logits to match student output logits length
            vis = _pad_or_truncate(vis, L1)
            chg = _pad_or_truncate(chg, L2)
            ans = _pad_or_truncate(ans, L3)

            # Concatenate all teacher logits for the question
            teacher_logits = torch.cat([vis, chg, ans], dim=0)
            full_tokenized['teacher_logits'] = teacher_logits

        return full_tokenized

def collate_fn(batch):
    collated = {}
    for key in batch[0].keys():
        sequences = [item[key] for item in batch]
        pad_val = -100 if key == "labels" else 0
        collated[key] = pad_sequence(sequences, batch_first=True, padding_value=pad_val)
    return collated

def compute_metrics(eval_preds):
    
    def flatten(x):
        # If x is a list with one element that is itself a list then return that sublist.
        if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list):
            print("STATUS: Found nested list")
            return x[0]
        return x

    print("-" * 80)
    print("STATUS: Started compute metrics after eval on eval_dataset")
    
    preds, labels = eval_preds

    # Check if preds or labels are torch.Tensor or np.ndarray and whether they contain raw logits.
    # For raw logits, we expect three dimensions: (batch_size, sequence_length, vocab_size).
    if isinstance(preds, (torch.Tensor, np.ndarray)):
        # Use .ndim if available.
        if preds.ndim == 3:
            print("STATUS: preds likely contains raw logits, applying argmax to convert to token IDs.")
            if isinstance(preds, torch.Tensor):
                preds = torch.argmax(preds, dim=-1)
            else:  # np.ndarray case
                preds = np.argmax(preds, axis=-1)
        else:
            print("STATUS: preds likely contains token IDs.")
    else:
        print("STATUS: preds is neither torch.Tensor nor np.ndarray; assuming token IDs.")

    # For labels, this is less common but checking in case they're raw logits.
    if isinstance(labels, (torch.Tensor, np.ndarray)):
        if labels.ndim == 3:
            print("STATUS: labels likely contains raw logits, applying argmax to convert to token IDs.")
            if isinstance(labels, torch.Tensor):
                labels = torch.argmax(labels, dim=-1)
            else:
                labels = np.argmax(labels, axis=-1)
        else:
            print("STATUS: labels likely contains token IDs.")
    else:
        print("STATUS: labels is neither torch.Tensor nor np.ndarray; assuming token IDs.")

    # Convert predictions to list-of-lists if necessary.
    print("STATUS: Convert preds to list of lists")
    if isinstance(preds, torch.Tensor):
        preds = preds.detach().cpu().tolist()
    elif isinstance(preds, np.ndarray):
        preds = preds.tolist()

    # Flatten preds to prevent nested list cases.
    print("STATUS: Flattening preds")
    preds = [flatten(p) for p in preds]

    # Replace masked labels (-100) with the pad token id and convert to list-of-lists if needed.
    print("STATUS: Replace masked tokens with token ids")
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
    if isinstance(labels, torch.Tensor):
        labels = labels.detach().cpu().tolist()
    elif isinstance(labels, np.ndarray):
        labels = labels.tolist()

    print("STATUS: Flattening labels")
    labels = [flatten(l) for l in labels] 

    # Inspecting input shape and type of elements in preds
    print("STATUS: Inspecting input shape and type of element in preds")
    for idx, p in enumerate(preds):
        print(f"preds[{idx}] type: {type(p)}; length: {len(p)}")

    # Decode preds and labels in chunks to avoid processing huge batches at once.
    print("STATUS: Decode preds and labels in chunks of 50")
    chunk_size = 50  # Adjust as needed
    decoded_preds = []
    for i in range(0, len(preds), chunk_size):
        print(f"STATUS: Processing chunk {i // chunk_size} of preds")
        decoded_preds.extend(processor.batch_decode(preds[i:i + chunk_size], skip_special_tokens=True))
    decoded_labels = []
    for i in range(0, len(labels), chunk_size):
        print(f"STATUS: Processing chunk {i // chunk_size} of labels")
        decoded_labels.extend(processor.batch_decode(labels[i:i + chunk_size], skip_special_tokens=True))

    # Helper function to ensure each element is a string.
    def safe_str(x):
        if not isinstance(x, str):
            try:
                x = str(x)
            except Exception:
                x = ""
        return x

    # Compute exact match score after decoding.
    print("STATUS: Compute EM after decoding preds, labels tokens to strings")
    exact_matches = [1 if safe_str(p).strip() == safe_str(l).strip() else 0 
                     for p, l in zip(decoded_preds, decoded_labels)]
    print("STATUS: Printing decoded_preds, decoded_labels")
    print(f"The decoded pred: {f}, the decoded label: {l}" for p, l in zip(decoded_preds, decoded_labels))
    exact_match_score = sum(exact_matches) / len(exact_matches)
    return {"exact_match": exact_match_score}

class DistillTrainer(Trainer):
    def __init__(self, alpha=0.5, temperature=1.0, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha
        self.temperature = temperature

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        teacher_logits = inputs.pop("teacher_logits", None)
        labels = inputs.get("labels")

        outputs = model(**inputs)
        student_logits = outputs.logits

        # check student logits are differentiable
        print("student_logits.requires_grad:", student_logits.requires_grad)

        loss_fn = CrossEntropyLoss(ignore_index=-100)
        ce_loss = loss_fn(student_logits.view(-1, student_logits.size(-1)),labels.view(-1))

        # Check if in training state as then only questions have teacher logits generated
        if teacher_logits is not None:
            B, T_s, V_s = student_logits.shape
            _, T_t, V_t = teacher_logits.shape

            # Align incompatible shape tensors
            # Align vocab size
            if V_t > V_s:
                teacher_logits = teacher_logits[..., :V_s]
            elif V_t < V_s:
                pad_vocab = torch.zeros((B, T_t, V_s - V_t), device=teacher_logits.device)
                teacher_logits = torch.cat([teacher_logits, pad_vocab], dim=-1)

            # Align time dimension
            if T_t < T_s:
                pad_time = torch.zeros((B, T_s - T_t, V_s), device=teacher_logits.device)
                teacher_logits = torch.cat([teacher_logits, pad_time], dim=1)
            else:
                teacher_logits = teacher_logits[:, :T_s, :]

            T = self.temperature
            s_log_probs = F.log_softmax(student_logits / T, dim=-1)
            t_probs = F.softmax(teacher_logits / T, dim=-1)

            # Mask out non-answer positions
            mask_tokens = (labels != -100)
            mask = mask_tokens.float().unsqueeze(-1) 
            s_log_probs = s_log_probs * mask
            t_probs = t_probs * mask

            # Compute KL divergence (batchmean) and scale by T^2
            kl_loss = F.kl_div(
                s_log_probs,
                t_probs,
                reduction="batchmean",
                log_target=False
            ) * (T * T)

        # Combine losses
        loss = self.alpha * ce_loss + (1 - self.alpha) * kl_loss

        print("student_logits.dtype:", student_logits.dtype)
        print("teacher_logits.dtype:", teacher_logits.dtype if teacher_logits is not None else None)
        print("ce_loss.requires_grad:", ce_loss.requires_grad)
        print("kl_loss.requires_grad:", kl_loss.requires_grad if teacher_logits is not None else None)
        print("final loss.requires_grad:", loss.requires_grad)

        if return_outputs:
            return loss, outputs
        return loss

import time

def main():
    # Clear GPU memory at start
    torch.cuda.empty_cache()
    gc.collect()

    print(f"Transformers version: {transformers.__version__}")

    parser = argparse.ArgumentParser()
    parser.add_argument("-f", "--train_data", type=str, default="../../hypo_dataset/train_contextvqa.json", help="Path to the training JSON file")
    parser.add_argument("-m", "--model_id", type=str, default="llava-hf/llava-onevision-qwen2-7b-ov-hf", help="Teacher model ID to finetune with LoRA")
    parser.add_argument("-i", "--images_dir", type=str, default="/gpfs/home/ym621/gavin/Hypo3D/hypo_dataset/dataset/top_view_with_label_rotated", help="Directory with scene images")
    parser.add_argument("-e", "--eval_data", type=str, default="../../hypo_dataset/val_contextvqa.json", help="Path to validation JSON file")
    parser.add_argument("-c", "--teacher_cache_dir", help="Folder with teacher model's .pt output logits files")
    parser.add_argument("-p", "--pipeline_run_dir", type=str, required=True, help="Directory to save finetuned model and outputs")
    parser.add_argument("-a", "--alpha_value", type=float, required=True, help="Alpha weightage value used in dual loss function calculation to weigh KL Div and CE loss")
    args = parser.parse_args()

    prompt_template = (
        "Given a top-view of a 3D scene, mentally rotate the image to align with the specified orientation.\n\n"
        "Scene Orientation: {}\n\n"
        "Now, given a context change, imagine how the scene would look after the change has been applied. Then, answer a question based on the changed scene.\n\n"
        "Context Change: {}\n"
        "Question: {}\n\n"
        "The answer should be a single word or short phrase.\n\n"
        "The answer is:"
    )

    visual_obs_template = (
    "Given a top-view of a 3D scene, mentally rotate the image to align with the specified orientation."

    "Scene Orientation: {orientation}"

    "Now, describe everything you see, listing each object and its location relative to the scene’s center (e.g. “The couch is at the back‐left corner, the plant is near the front‐right edge)."

    )

    context_change_template = '''
    The visual observations you determined for the scene are: {model_vis_obs}.

    Now, given a context change, describe how the scene looks after the change has been applied. List any changed or removed objects and their positions.

    Context Change: {change}

    '''

    question_template = '''
    Answer this question: 

    Based on the visual observations: {model_vis_obs}

    Context change: {change}

    Scene after change: {context_change_obs}

    Question: {question}

    The answer should be a single word or short phrase.

    The answer is:
    '''

    global processor
    processor = AutoProcessor.from_pretrained(args.model_id)
    model = LlavaOnevisionForConditionalGeneration.from_pretrained(
        args.model_id, 
        use_flash_attention_2=False,
        device_map="auto"
    )

    model.config.use_cache = False

    print("-" * 80)
    print("INFO: Printing signature of model foward func")

    import inspect
    print(inspect.signature(model.forward))
    print(model.forward.__doc__)
    print("-" * 80)

    for name, module in model.named_modules():
        print(name)
        
    num_vision_encoder_layers = 26
    vision_modules = []
    for i in range(num_vision_encoder_layers):
        for proj in ["k_proj", "v_proj", "q_proj"]:
            vision_modules.append(f"vision_tower.vision_model.encoder.layers.{i}.self_attn.{proj}")

    num_language_model_layers = 24
    language_modules = []
    for i in range(num_language_model_layers):
        for proj in ["k_proj", "v_proj", "q_proj"]:
            language_modules.append(f"language_model.model.layers.{i}.self_attn.{proj}")

    target_modules = language_modules + vision_modules

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=target_modules,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    df = pd.read_excel("/gpfs/home/ym621/gavin/Hypo3D/hypo_dataset/axis_def_hypo.xlsx", sheet_name='Sheet1', engine='openpyxl')
    # df = pd.read_excel("/rds/general/user/ym621/home/gavin/Hypo3D/hypo_dataset/axis_def_hypo.xlsx", sheet_name='Sheet1', engine='openpyxl')
    train_dataset = Dataset(args.train_data, args.images_dir, processor, prompt_template, visual_obs_template, context_change_template, question_template, df, args.teacher_cache_dir, split="train")
    eval_dataset = Dataset(args.eval_data, args.images_dir, processor, prompt_template, visual_obs_template, context_change_template, question_template, df, args.teacher_cache_dir, split="eval")

    training_args = TrainingArguments(
        # output_dir="lora_finetuned_llava",
        output_dir="lora_distill",
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=1,
        eval_accumulation_steps=4,
        learning_rate=5e-5,
        logging_steps=10,
        save_steps=100,
        eval_strategy="steps",
        eval_steps=50,
        # eval_strategy="no",
        fp16=True,
        gradient_accumulation_steps=4,
        report_to="wandb",
        run_name="cot_lora_finetuning_student_model_llava_ov_0_5b",
        label_names=["labels"],
        optim="adamw_torch",
    )

    # trainer = Trainer(
    #     model=model,
    #     args=training_args,
    #     train_dataset=train_dataset,
    #     eval_dataset=eval_dataset,
    #     data_collator=collate_fn,
    #     compute_metrics=compute_metrics,
    # )

        # instantiate our DistillTrainer
    trainer = DistillTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collate_fn,
        compute_metrics=compute_metrics,   # or your existing compute_metrics
        # alpha=0.5,
        alpha=args.alpha_value,
        temperature=1.0
    )

    trainer.train()
    model_name = args.model_id.replace("/", "_")
    full_path = os.path.join(args.pipeline_run_dir, f"lora_distilled_{model_name}")
    os.makedirs(full_path, exist_ok=True)
    model.save_pretrained(full_path)
    print(f"Distilled LoRA adaptser saved to: {full_path}")

if __name__ == "__main__":
    main()
